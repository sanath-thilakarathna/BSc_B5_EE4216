{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84e9e3d",
   "metadata": {},
   "source": [
    "# Geometric Transformations & Image Registration\n",
    "## 1. Introduction\n",
    "**Lecture objectives**\n",
    "- Understand common geometric transforms and when to use them.\n",
    "- Apply affine and perspective transforms with OpenCV.\n",
    "- Recognize the role of registration in aligning images.\n",
    "\n",
    "**Overview**\n",
    "Geometric transformations are fundamental operations in computer vision that modify the spatial layout of images. They include:\n",
    "- **Affine transforms**: Preserve parallelism (rotation, translation, scaling, shearing).\n",
    "- **Perspective transforms**: General projective transforms that handle viewing angle changes.\n",
    "- **Registration**: Aligning multiple images to a common coordinate system for comparison or fusion.\n",
    "- **Stitching**: Combining overlapping images to create wide-angle panoramas.\n",
    "\n",
    "These techniques are essential for applications like medical imaging alignment, panorama creation, document scanning, and video stabilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113c7e8",
   "metadata": {},
   "source": [
    "**Required libraries**\n",
    "- `numpy` for array math\n",
    "- `opencv-python` (`cv2`) for image transforms\n",
    "- `matplotlib` for quick visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f71d0",
   "metadata": {},
   "source": [
    "**Image data structure (NumPy array basics)**\n",
    "- Grayscale image: 2D array with shape `(H, W)`.\n",
    "- Color image (BGR in OpenCV): 3D array with shape `(H, W, 3)`.\n",
    "- Pixel values are typically `uint8` in `[0, 255]` for 8-bit images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfe1ee",
   "metadata": {},
   "source": [
    "## 2. Image I/O and Visualization\n",
    "**Key concepts**\n",
    "- **imread**: Load images from disk (BGR format in OpenCV, unlike standard RGB).\n",
    "- **cvtColor**: Convert between color spaces (BGR, RGB, HSV, Grayscale, LAB).\n",
    "- **imwrite**: Save processed images to disk.\n",
    "- **Camera capture**: Real-time image acquisition from webcams or video files.\n",
    "\n",
    "**Why OpenCV uses BGR**\n",
    "OpenCV defaults to **BGR (Blue-Green-Red)** component ordering instead of the standard RGB. This arose historically from Intel's image processing conventions. When working with images from other libraries (e.g., Matplotlib expects RGB), you must explicitly convert using `cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`. This is critical for correct color visualization and algorithm performance—imagine applying a blue-channel de-blur filter to what you think is the red channel.\n",
    "\n",
    "**Color space selection guide**\n",
    "- **BGR/RGB**: General-purpose color images; good for display.\n",
    "- **Grayscale**: Reduces computation (1 channel vs 3); used when color is irrelevant (e.g., edge detection, feature matching).\n",
    "- **HSV**: Hue-Saturation-Value; intuitive for human vision; excellent for color-based segmentation (detecting 'redness' independent of lighting).\n",
    "- **LAB**: Perceptually uniform; used in medical imaging and cross-domain registration where color perception matters.\n",
    "\n",
    "**Live camera input**\n",
    "Capturing real-time video enables interactive applications: camera calibration, live effect demonstration, or augmented reality. Always release the camera with `cap.release()` to avoid resource leaks or conflicts with other applications.\n",
    "\n",
    "**Application context**: Image I/O forms the pipeline foundation. Medical imaging software loads DICOM files and converts to standard color spaces before registration. Document scanners capture live frames and convert to grayscale for OCR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: read an image to inspect size and dtype before any processing\n",
    "# Why: many algorithms assume a consistent shape and 8-bit data\n",
    "img_path = \"img/cleon.jpg\"\n",
    "img_bgr = cv2.imread(img_path)\n",
    "# Where: dataset inspection and preprocessing pipelines\n",
    "if img_bgr is None:\n",
    "    raise FileNotFoundError(f\"Could not read {img_path}\")\n",
    "\n",
    "# Inspect array shape and dtype to confirm expectations\n",
    "print(\"shape:\", img_bgr.shape)\n",
    "print(\"dtype:\", img_bgr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: convert BGR to RGB for correct notebook display\n",
    "# Why: matplotlib expects RGB, but OpenCV loads BGR\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Where: quick visualization during analysis and debugging\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"RGB display via matplotlib\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Script-based display (won't show inside most notebooks)\n",
    "# cv2.imshow(\"BGR image\", img_bgr)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36957b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: open camera and capture a frame\n",
    "# Why: live image capture for real-time processing and testing\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open camera\")\n",
    "\n",
    "# Capture a single frame\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to capture frame from camera\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Display the captured frame\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(frame_rgb)\n",
    "plt.title(\"Camera capture\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: show multiple images in one figure for comparison\n",
    "# Why: side-by-side viewing highlights differences quickly\n",
    "img2_bgr = cv2.imread(\"img/millennium-falcon.jpg\")\n",
    "img3_bgr = cv2.imread(\"img/the-vault.jpg\")\n",
    "# Where: dataset review and reporting\n",
    "if img2_bgr is None or img3_bgr is None:\n",
    "    raise FileNotFoundError(\"Could not read one of the extra images\")\n",
    "\n",
    "# Convert to RGB for matplotlib\n",
    "imgs_rgb = [img_rgb, cv2.cvtColor(img2_bgr, cv2.COLOR_BGR2RGB), cv2.cvtColor(img3_bgr, cv2.COLOR_BGR2RGB)]\n",
    "titles = [\"cleon\", \"millennium-falcon\", \"the-vault\"]\n",
    "\n",
    "# Plot side-by-side in one row\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(imgs_rgb[i])\n",
    "    plt.title(titles[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: save a derived image to disk\n",
    "# Why: share results or build a processed dataset\n",
    "out_path = \"img/cleon_gray.png\"\n",
    "img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(out_path, img_gray)\n",
    "print(\"saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b511f8",
   "metadata": {},
   "source": [
    "## 3. Drawing and Annotation\n",
    "**Key functions**\n",
    "- Drawing primitives: line, circle, rectangle, ellipse, polylines.\n",
    "- Text overlay: putText with font selection and baseline adjustment.\n",
    "- Region of Interest (ROI): Selecting sub-regions for processing or analysis.\n",
    "\n",
    "**Why annotation matters**\n",
    "Visualization is essential for debugging and communication in computer vision. Overlaying detected features, bounding boxes, or trajectory paths on images helps you verify algorithm correctness (\"Did my edge detector work?\"), communicate results to stakeholders, and prepare publication-quality figures. Color choice matters: use contrasting colors against the image background (e.g., green lines on complex scenes, blue on bright backgrounds).\n",
    "\n",
    "**Practical uses**\n",
    "- **Detection validation**: Draw bounding boxes around detected objects to confirm the detector is working.\n",
    "- **Feature visualization**: Mark keypoints with circles to show where SIFT/ORB found interest points.\n",
    "- **Interactive ROI selection**: Allow users to draw rectangles for region-based processing (e.g., histogram equalization in a selected area).\n",
    "- **Augmented reality**: Overlay computed overlays (transformed chess boards, directional arrows) to demonstrate geometric accuracy.\n",
    "- **Medical imaging**: Annotate regions of interest, measurements, or pathological findings.\n",
    "\n",
    "**Application context**: In panorama stitching, you might annotate matched feature pairs to verify matching quality before homography estimation. In document scanning, you draw the detected document outline before perspective correction to show the algorithm's perception of the page boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3826a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: draw shapes and labels on top of an image\n",
    "# Why: visualize ROIs, annotations, or measurement overlays\n",
    "canvas = img_bgr.copy()\n",
    "\n",
    "# Get image size for relative placement\n",
    "h, w = canvas.shape[:2]\n",
    "\n",
    "# Draw primitives for annotation and debugging\n",
    "cv2.line(canvas, (20, 20), (w - 20, 20), (0, 255, 0), 2)\n",
    "cv2.circle(canvas, (w // 4, h // 2), 40, (255, 0, 0), 2)\n",
    "cv2.rectangle(canvas, (w // 2 - 60, h // 2 - 40), (w // 2 + 60, h // 2 + 40), (0, 0, 255), 2)\n",
    "cv2.ellipse(canvas, (w - 100, h - 80), (60, 30), 30, 0, 360, (255, 255, 0), 2)\n",
    "\n",
    "# Define polygon vertices and draw a closed shape\n",
    "poly_pts = np.array([[50, h - 60], [120, h - 100], [180, h - 60], [120, h - 20]], dtype=np.int32)\n",
    "cv2.polylines(canvas, [poly_pts], isClosed=True, color=(0, 255, 255), thickness=2)\n",
    "\n",
    "# Add label text for clarity\n",
    "cv2.putText(canvas, \"OpenCV\", (20, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "# Where: UI overlays, labeling tools, and demo visuals\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Drawing primitives\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff28cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: detect and visualize local features (keypoints)\n",
    "# Why: keypoints support matching, tracking, and registration\n",
    "orb = cv2.ORB_create(nfeatures=300)\n",
    "# Detect keypoints and descriptors in the image\n",
    "kps, desc = orb.detectAndCompute(img_bgr, None)\n",
    "# Draw keypoints to see coverage and scale\n",
    "kp_vis = cv2.drawKeypoints(img_bgr, kps, None, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Where: feature-based alignment and stitching\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(cv2.cvtColor(kp_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Keypoints (ORB)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: match features between two images\n",
    "# Why: correspondences are needed for registration and stitching\n",
    "img2_bgr = cv2.imread(\"img/millennium-falcon.jpg\")\n",
    "if img2_bgr is None:\n",
    "    raise FileNotFoundError(\"Could not read img/millennium-falcon.jpg\")\n",
    "\n",
    "# Detect keypoints and descriptors in the second image\n",
    "kps2, desc2 = orb.detectAndCompute(img2_bgr, None)\n",
    "# Guard against missing descriptors to avoid matcher errors\n",
    "if desc is None or desc2 is None:\n",
    "    raise ValueError(\"No descriptors found for matching\")\n",
    "\n",
    "# Match descriptors with brute-force Hamming distance (ORB uses binary descriptors)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = sorted(bf.match(desc, desc2), key=lambda m: m.distance)\n",
    "# Draw top matches for visual inspection\n",
    "match_vis = cv2.drawMatches(img_bgr, kps, img2_bgr, kps2, matches[:20], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Where: panorama stitching, pose estimation, and registration\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv2.cvtColor(match_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Feature matches (ORB + BFMatcher)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b870d",
   "metadata": {},
   "source": [
    "## 4. Basic Geometric Transformations\n",
    "**Mathematical foundation**\n",
    "Geometric transforms are expressed as matrix operations on image coordinates. In homogeneous coordinates, a 2D point $(x, y)$ becomes $(x, y, 1)$, allowing translation to be expressed as matrix multiplication. A 2×3 matrix $M$ transforms a point via:\n",
    "$\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = M \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "**When to use each transform**\n",
    "- **Translation**: Shifting images to align objects or correct misalignment from uncontrolled camera motion.\n",
    "- **Rotation**: Correcting image orientation (camera tilted 30°), aligning rotated documents, or rotating feature-extracted patches for orientation-invariant analysis.\n",
    "- **Scaling**: Changing resolution, zooming into regions of interest, or normalizing image sizes for batch processing.\n",
    "- **Combined transforms**: Real-world scenarios rarely involve single operations; most camera motions combine rotation, translation, and scale.\n",
    "- **Border handling**: Choice of constants, replication, or reflection affects how algorithms process image edges (important for edge-aware filters).\n",
    "\n",
    "**Interpolation quality trade-offs**\n",
    "- **INTER_NEAREST**: Fast but blocky; use only when speed dominates quality (e.g., real-time preview).\n",
    "- **INTER_LINEAR**: Fast and smooth; good general-purpose choice for most applications.\n",
    "- **INTER_CUBIC**: Slower but smoother; use for publication-quality results or when downsampling significantly.\n",
    "- **INTER_LANCZOS4**: Highest quality but slowest; use for final output or small, precious datasets.\n",
    "\n",
    "**Application context**: Correcting camera tilt in document scanning requires combined rotation + scaling. Augmented reality requires real-time transform updates as the camera moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: shift the image by a fixed offset (translation)\n",
    "# Why: align objects or compensate for camera motion\n",
    "tx, ty = 60, 30\n",
    "M_trans = np.array([[1, 0, tx], [0, 1, ty]], dtype=np.float32)\n",
    "# Apply translation; output size (w, h) is required\n",
    "img_trans = cv2.warpAffine(img_bgr, M_trans, (w, h))\n",
    "\n",
    "# Where: registration and tracking pipelines\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(cv2.cvtColor(img_trans, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Translation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91e0ee",
   "metadata": {},
   "source": [
    "### 4.1 Translation\n",
    "Shift an image by a fixed offset. Useful for aligning images that were captured from slightly different positions. **Goal**: Demonstrate 2D translation matrix and pixel shifting logic. **Why**: Translation is the simplest transform; understanding it establishes the matrix-based approach used for all subsequent transforms. **Where**: Scene alignment, camera stabilization, object tracking (shifting template to match target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: rotate the image around its center\n",
    "# Why: normalize orientation differences\n",
    "center = (w // 2, h // 2)\n",
    "angle = 25\n",
    "scale = 1.0\n",
    "M_rot = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "# Apply rotation using the affine matrix\n",
    "img_rot = cv2.warpAffine(img_bgr, M_rot, (w, h))\n",
    "\n",
    "# Where: document alignment and robust matching\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(cv2.cvtColor(img_rot, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Rotation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127552c",
   "metadata": {},
   "source": [
    "### 4.2 Rotation\n",
    "Rotate an image around a center point by an angle. Rotation changes both position and orientation of features. **Goal**: Show 2D rotation matrix composition and interpolation artifacts. **Why**: Rotation is fundamental for correcting tilted images and aligning patterns with specific orientations. **Where**: Correcting scanned document tilt, aligning aerial imagery, rotating face landmarks detected at arbitrary angles, panoramic image creation (aligning frames captured at different angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4025d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: resize with different interpolation strategies\n",
    "# Why: speed/quality tradeoffs depend on interpolation choice\n",
    "scale_x, scale_y = 0.6, 0.6\n",
    "new_size = (int(w * scale_x), int(h * scale_y))\n",
    "\n",
    "# Nearest is fastest, linear is common, cubic is smoother but slower\n",
    "img_nearest = cv2.resize(img_bgr, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "img_linear = cv2.resize(img_bgr, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "img_cubic = cv2.resize(img_bgr, new_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Where: model input sizing and multi-scale analysis\n",
    "plt.figure(figsize=(9, 3))\n",
    "for i, (im, title) in enumerate([(img_nearest, \"NEAREST\"), (img_linear, \"LINEAR\"), (img_cubic, \"CUBIC\")], start=1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bb3a0",
   "metadata": {},
   "source": [
    "### 4.3 Scaling\n",
    "Enlarge or shrink an image uniformly or non-uniformly (aspect-change). **Goal**: Demonstrate uniform (isotropic) and non-uniform (anisotropic) scaling. **Why**: Scaling is essential for multi-scale analysis (detecting objects at multiple scales), normalizing input to fixed dimensions, and zooming into regions of interest. Losing aspect ratio can distort face proportions or document text. **Where**: Thumbnail generation, batch image normalization, multi-scale feature detection (SIFT pyramid), video frame resizing to match processing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: compose transforms to apply once\n",
    "# Why: reduces repeated interpolation artifacts\n",
    "tx2, ty2 = -40, 20\n",
    "M_t = np.array([[1, 0, tx2], [0, 1, ty2]], dtype=np.float32)\n",
    "M_r = cv2.getRotationMatrix2D(center, -15, 1.0)\n",
    "\n",
    "# Convert 2x3 affine matrices to 3x3 for composition\n",
    "M_t3 = np.vstack([M_t, [0, 0, 1]])\n",
    "M_r3 = np.vstack([M_r, [0, 0, 1]])\n",
    "# Compose: first rotate, then translate\n",
    "M_comp = M_t3 @ M_r3\n",
    "\n",
    "# Convert back to 2x3 for warpAffine\n",
    "M_comp2x3 = M_comp[:2, :]\n",
    "img_comp = cv2.warpAffine(img_bgr, M_comp2x3, (w, h))\n",
    "\n",
    "# Where: stabilized video frames and aligned augmentation\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(cv2.cvtColor(img_comp, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Combined: rotation + translation\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124ea56",
   "metadata": {},
   "source": [
    "### 4.4 Combined Transformations\n",
    "Apply multiple transforms sequentially: rotation, then scaling, then translation. **Goal**: Illustrate matrix composition where order matters ($M_{final} = M_{trans} \\times M_{scale} \\times M_{rot}$). **Why**: Real-world camera motions involve rotation AND scale AND translation simultaneously. Proper matrix composition ensures all transforms apply correctly. **Where**: Simulating camera movements in video, aligning images taken from different viewpoints and distances, augmented reality object placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc03a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: show how border rules affect warped images\n",
    "# Why: edges can introduce artifacts in filtering/warping\n",
    "M_shift = np.array([[1, 0, 80], [0, 1, 0]], dtype=np.float32)\n",
    "\n",
    "# Apply the same shift with different border modes\n",
    "img_const = cv2.warpAffine(img_bgr, M_shift, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "img_reflect = cv2.warpAffine(img_bgr, M_shift, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "img_replicate = cv2.warpAffine(img_bgr, M_shift, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# Where: warping, filtering, and registration pipelines\n",
    "plt.figure(figsize=(9, 3))\n",
    "for i, (im, title) in enumerate([(img_const, \"CONSTANT\"), (img_reflect, \"REFLECT\"), (img_replicate, \"REPLICATE\")], start=1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49165864",
   "metadata": {},
   "source": [
    "### 4.5 Border Handling Modes\n",
    "When a transform shifts pixels outside the original image bounds, how should those new areas be filled? OpenCV offers: **BORDER_CONSTANT** (black padding), **BORDER_REPLICATE** (repeat edge pixels), **BORDER_REFLECT** (mirror image). **Goal**: Demonstrate how border choice affects edge processing. **Why**: Edge-aware algorithms may be sensitive to boundary values. Constant black borders can introduce false edges; replication preserves continuity. **Where**: Document scanning (replicate corners to maintain edges), removing letterboxes from video (constant), image blending where feathering is needed (reflect avoids sharp edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b29988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: estimate and apply an affine transform from 3 point pairs\n",
    "# Why: affine captures rotation, translation, scale, and shear\n",
    "src_pts = np.float32([[50, 50], [w - 60, 60], [60, h - 60]])\n",
    "dst_pts = np.float32([[30, 80], [w - 80, 40], [80, h - 80]])\n",
    "\n",
    "# Compute affine transform matrix (2x3)\n",
    "M_aff = cv2.getAffineTransform(src_pts, dst_pts)\n",
    "# Apply affine transform to the image\n",
    "img_aff = cv2.warpAffine(img_bgr, M_aff, (w, h))\n",
    "\n",
    "# Visualize mapping by marking source and target points\n",
    "src_vis = img_bgr.copy()\n",
    "dst_vis = img_aff.copy()\n",
    "for pt in src_pts:\n",
    "    cv2.circle(src_vis, tuple(pt.astype(int)), 6, (0, 255, 0), -1)\n",
    "for pt in dst_pts:\n",
    "    cv2.circle(dst_vis, tuple(pt.astype(int)), 6, (0, 255, 0), -1)\n",
    "\n",
    "# Where: image registration and rectification tasks\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(src_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Source points\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(dst_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Transformed image + target points\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012229b",
   "metadata": {},
   "source": [
    "## 5. Affine Transformation\n",
    "**Theory**: An affine transformation preserves parallel lines and is defined by mapping 3 source points to 3 destination points. The resulting 2×3 transformation matrix encodes rotation, scaling, translation, and shearing in a single operation. **Goal**: Estimate affine matrix from point correspondences and apply it to an image. **Why**: Affine transforms model planar camera motions (viewing scene from slightly different angles) and are computationally efficient compared to perspective transforms. **Where**: Document perspective correction if viewed at a shallow angle, face alignment (warping detected landmarks to canonical positions), medical image registration (aligning scans from different angles). **Practical note**: Require exactly 3 well-distributed point pairs to be uniquely determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ab983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: map a quadrilateral region to a rectangle (bird's-eye view)\n",
    "# Why: correct perspective distortion on planar surfaces\n",
    "# Select four source points in the original image\n",
    "src4 = np.float32([[80, 80], [w - 80, 60], [w - 60, h - 80], [70, h - 60]])\n",
    "# Define destination rectangle corners\n",
    "dst_w, dst_h = w, h\n",
    "dst4 = np.float32([[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]])\n",
    "\n",
    "# Compute homography (3x3) and warp perspective\n",
    "M_persp = cv2.getPerspectiveTransform(src4, dst4)\n",
    "img_persp = cv2.warpPerspective(img_bgr, M_persp, (dst_w, dst_h))\n",
    "\n",
    "# Visualize mapping: draw source quad on original\n",
    "src_quad_vis = img_bgr.copy()\n",
    "cv2.polylines(src_quad_vis, [src4.astype(int)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "# Where: bird's-eye views for documents and roads\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(src_quad_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Source quadrilateral\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(img_persp, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Warped bird's-eye view\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc62662",
   "metadata": {},
   "source": [
    "## 7. Advanced Warping & Remapping\n",
    "- `cv2.remap()` applies a per-pixel mapping for flexible warps.\n",
    "- `cv2.invertAffineTransform()` computes the inverse of a $2 \\times 3$ affine matrix.\n",
    "**Why it matters**\n",
    "- Remapping enables custom distortions and coordinate corrections.\n",
    "**Where it applies**\n",
    "- Lens correction, undistortion, and geometry-based alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: use remap to apply a custom warp (horizontal ripple)\n",
    "# Why: remap gives per-pixel control over where each output pixel samples\n",
    "map_x, map_y = np.meshgrid(np.arange(w, dtype=np.float32), np.arange(h, dtype=np.float32))\n",
    "\n",
    "# Create a small sinusoidal horizontal shift based on y\n",
    "shift = 12.0 * np.sin(2 * np.pi * map_y / 120.0)\n",
    "map_x_warp = map_x + shift\n",
    "map_y_warp = map_y.copy()\n",
    "\n",
    "# Apply remap (border reflects to avoid black edges)\n",
    "img_remap = cv2.remap(img_bgr, map_x_warp, map_y_warp, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "# Where: lens correction, creative effects, and calibration pipelines\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(img_remap, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Remap (ripple)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7161c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: invert an affine transform and map back to original space\n",
    "# Why: useful when you need to undo a transform or map coordinates back\n",
    "M_inv = cv2.invertAffineTransform(M_trans)\n",
    "img_unshift = cv2.warpAffine(img_trans, M_inv, (w, h))\n",
    "\n",
    "# Where: registration pipelines and coordinate back-projection\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_trans, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Translated\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(img_unshift, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"After inverse transform\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56501d0d",
   "metadata": {},
   "source": [
    "## 8. Feature Detection\n",
    "**Theory**: Features (keypoints) are distinctive image regions at multiple scales. Corner detectors (FAST, Harris) find intensity discontinuities; blob detectors (ORB, SIFT) find circular regions of interest. Each keypoint is described by a feature descriptor—a compact vector encoding local appearance. **Goal**: Detect ORB and SIFT features; compare by region count. **Why**: Features are repeatably detectable across image variations (rotation, scale, illumination). They form the foundation for image matching. **Where**: Structure-from-motion (aligning image sequences), object recognition (matching templates in scenes), image stitching (finding overlaps), content-based image retrieval, robot localization using loop closure detection. **Practical comparison**: ORB is fast (real-time), rotation-invariant, but less precise in low-texture regions. SIFT is slower, highly distinctive, scale-invariant, but patented (use with caution in commercial code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc124183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: extract keypoints/descriptors with ORB and (optionally) SIFT\n",
    "# Why: descriptors enable matching between images for registration\n",
    "orb_fd = cv2.ORB_create(nfeatures=500)\n",
    "orb_kps, orb_desc = orb_fd.detectAndCompute(img_bgr, None)\n",
    "print(\"ORB keypoints:\", len(orb_kps))\n",
    "print(\"ORB descriptor shape:\", None if orb_desc is None else orb_desc.shape)\n",
    "\n",
    "# Try SIFT if available in the OpenCV build\n",
    "if hasattr(cv2, \"SIFT_create\"):\n",
    "    sift = cv2.SIFT_create()\n",
    "    sift_kps, sift_desc = sift.detectAndCompute(img_bgr, None)\n",
    "    print(\"SIFT keypoints:\", len(sift_kps))\n",
    "    print(\"SIFT descriptor shape:\", None if sift_desc is None else sift_desc.shape)\n",
    "else:\n",
    "    print(\"SIFT not available in this OpenCV build\")\n",
    "\n",
    "# Where: feature matching and geometric verification pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32576e",
   "metadata": {},
   "source": [
    "## 7. Advanced Warping Techniques\n",
    "**Theory**: Beyond standard transforms, arbitrary pixel-level remapping (using coordinate lookup tables) and transform inversion enable specialized effects and techniques. **Remapping** applies per-pixel displacement without global geometric structure. **Inversion** reverses a transform (useful for undoing detected distortions). **Goal**: Apply ripple effects via remapping; invert transforms to 'unwarp' images. **Why**: Remapping is flexible for non-rigid deformations (water ripples, lens distortion correction); inversion is crucial for undoing known camera calibration issues. **Where**: Correcting barrel distortion in fisheye lenses, ripple/water effects in filters, reversing geometric distortions caused by camera movement, medical image unwarping (unwrapping cylindrical vessel walls in CT scans for analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: match ORB descriptors with brute-force matching\n",
    "# Why: ORB uses binary descriptors suited to Hamming distance\n",
    "if orb_desc is None or desc2 is None:\n",
    "    raise ValueError(\"Descriptors missing for matching\")\n",
    "\n",
    "# BFMatcher with Hamming for ORB; match() gives the best match per descriptor\n",
    "bf_orb = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "bf_matches = sorted(bf_orb.match(orb_desc, desc2), key=lambda m: m.distance)\n",
    "print(\"BF matches:\", len(bf_matches))\n",
    "\n",
    "# Visualize a few matches\n",
    "bf_vis = cv2.drawMatches(img_bgr, orb_kps, img2_bgr, kps2, bf_matches[:20], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv2.cvtColor(bf_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"BFMatcher (ORB)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0489100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: use k-NN matching + ratio test for cleaner matches\n",
    "# Why: ratio test removes ambiguous matches with similar distances\n",
    "# Use BFMatcher without crossCheck to allow k-NN\n",
    "bf_knn = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "knn_matches = bf_knn.knnMatch(orb_desc, desc2, k=2)\n",
    "\n",
    "# Apply Lowe's ratio test\n",
    "ratio = 0.75\n",
    "good = []\n",
    "for m, n in knn_matches:\n",
    "    if m.distance < ratio * n.distance:\n",
    "        good.append(m)\n",
    "print(\"KNN matches:\", len(knn_matches), \"Good after ratio:\", len(good))\n",
    "\n",
    "# Visualize ratio-test matches\n",
    "knn_vis = cv2.drawMatches(img_bgr, orb_kps, img2_bgr, kps2, good[:30], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv2.cvtColor(knn_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"KNN + ratio test (ORB)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: use FLANN-based matcher (fast approximate)\n",
    "# Why: faster for large descriptor sets; works well with SIFT\n",
    "if 'sift_desc' in locals() and sift_desc is not None:\n",
    "    # Need SIFT descriptors for the second image too\n",
    "    if hasattr(cv2, \"SIFT_create\"):\n",
    "        sift2 = cv2.SIFT_create()\n",
    "        sift_kps2, sift_desc2 = sift2.detectAndCompute(img2_bgr, None)\n",
    "        \n",
    "        if sift_desc2 is not None:\n",
    "            # Create FLANN matcher for SIFT (float descriptors)\n",
    "            FLANN_INDEX_KDTREE = 1\n",
    "            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "            search_params = dict(checks=50)\n",
    "            flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "            \n",
    "            # Match with k=2 for ratio test\n",
    "            flann_knn = flann.knnMatch(sift_desc, sift_desc2, k=2)\n",
    "            \n",
    "            # Ratio test for SIFT matches\n",
    "            good_flann = []\n",
    "            for m, n in flann_knn:\n",
    "                if m.distance < 0.75 * n.distance:\n",
    "                    good_flann.append(m)\n",
    "            print(\"FLANN good matches:\", len(good_flann))\n",
    "\n",
    "            # Visualize a subset\n",
    "            flann_vis = cv2.drawMatches(img_bgr, sift_kps, img2_bgr, sift_kps2, good_flann[:30], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(cv2.cvtColor(flann_vis, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"FLANN + ratio test (SIFT)\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"SIFT descriptors not found for second image; skipping FLANN demo\")\n",
    "    else:\n",
    "        print(\"SIFT not available in this OpenCV build; skipping FLANN demo\")\n",
    "else:\n",
    "    print(\"SIFT not available or descriptors missing; skipping FLANN demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfd481",
   "metadata": {},
   "source": [
    "## 9. Feature Matching\n",
    "**Theory**: Given keypoints and descriptors in two images, matching finds corresponding points. Simple matching returns the nearest neighbor by descriptor distance. **Lowe's ratio test** improves accuracy by rejecting matches where the nearest and 2nd-nearest descriptors are too close (ambiguous). FLANN (Fast Library for Approximate Nearest Neighbors) uses space-partitioning trees for fast matching. **Goal**: Demonstrate basic BFMatcher, ratio-test filtering, and FLANN-based matching. **Why**: Robust matching reduces false positives (spurious correspondences leading to incorrect transforms). The ratio test is statistically principled: if the best match is only slightly better than the second-best, the match is unreliable. **Where**: Image stitching (finding overlapping regions), object recognition (matching templates), localization (find where an object appears in a scene), medical image registration (aligning patient scans). **Trade-off**: Ratio test reduces false positives but may discard some true matches in ambiguous regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: estimate a homography from matched keypoints using RANSAC\n",
    "# Why: robustly fit a projective model while rejecting outliers\n",
    "# Use ORB keypoints from Section 8 and good matches from Section 9\n",
    "\n",
    "# Ensure we have good matches from the ratio test (cell 34)\n",
    "if 'good' not in locals() or len(good) < 4:\n",
    "    # Fallback: use brute-force matches if ratio test matches are insufficient\n",
    "    if 'bf_matches' in locals() and len(bf_matches) >= 4:\n",
    "        good = bf_matches\n",
    "    else:\n",
    "        raise ValueError(\"Need at least 4 good matches to compute homography\")\n",
    "\n",
    "if len(good) < 4:\n",
    "    raise ValueError(\"Need at least 4 good matches to compute homography\")\n",
    "\n",
    "# Build point arrays for homography estimation\n",
    "src_pts_h = np.float32([orb_kps[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "dst_pts_h = np.float32([kps2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography with RANSAC to separate inliers/outliers\n",
    "H, mask = cv2.findHomography(src_pts_h, dst_pts_h, cv2.RANSAC, 5.0)\n",
    "if H is None:\n",
    "    raise ValueError(\"Homography estimation failed\")\n",
    "\n",
    "# Count inliers vs outliers\n",
    "inlier_mask = mask.ravel().astype(bool)\n",
    "num_inliers = int(inlier_mask.sum())\n",
    "num_outliers = int(len(inlier_mask) - num_inliers)\n",
    "print(\"Inliers:\", num_inliers, \"Outliers:\", num_outliers)\n",
    "\n",
    "# Visualize inlier matches only\n",
    "inlier_matches = [m for m, keep in zip(good, inlier_mask) if keep]\n",
    "H_vis = cv2.drawMatches(img_bgr, orb_kps, img2_bgr, kps2, inlier_matches[:30], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Where: panorama stitching and planar alignment\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv2.cvtColor(H_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Inlier matches after RANSAC\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7258377",
   "metadata": {},
   "source": [
    "## 10. Homography Estimation with RANSAC\n",
    "**Theory**: RANSAC (Random Sample Consensus) is a robust estimation algorithm that finds geometric transforms despite abundant outliers. It repeatedly: (1) samples minimal sets (4 points for homography), (2) estimates a transform, (3) counts inliers within a threshold, (4) keeps the best model. This separates correct correspondences (inliers) from mismatches (outliers). **Goal**: Estimate homography; segment inliers/outliers; visualize them separately. **Why**: Feature matching produces false positives, especially near image boundaries or repetitive textures. RANSAC is statistically principled and handles high outlier ratios (even 80%+). **Where**: Panorama stitching (handling matched features with some jitter), object recognition in cluttered scenes, video stabilization, document detection in natural photos. **Practical insight**: The inlier threshold choice affects sensitivity; too loose includes noise, too tight rejects valid matches. Standard choice: median re-projection error $\\times$ 2-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: align img_bgr to img2_bgr using the estimated homography\n",
    "# Why: registration puts images into a common coordinate system\n",
    "if 'H' not in locals() or H is None:\n",
    "    raise ValueError(\"Homography H not available; run Section 10 first\")\n",
    "\n",
    "# Warp img_bgr into the coordinate frame of img2_bgr\n",
    "h2, w2 = img2_bgr.shape[:2]\n",
    "img_reg = cv2.warpPerspective(img_bgr, H, (w2, h2))\n",
    "\n",
    "# Visual validation: overlay aligned image with the reference\n",
    "overlay = cv2.addWeighted(img_reg, 0.5, img2_bgr, 0.5, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_reg, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Warped to reference\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Overlay (validation)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149b98a",
   "metadata": {},
   "source": [
    "## 11. Image Registration\n",
    "**Theory**: Registration aligns images to a common coordinate system, enabling comparison or fusion. Given matched features and a robust homography, the estimated transform moves one image onto another. **Goal**: Apply estimated homography to warp an image; overlay to visualize alignment. **Why**: Medical imaging requires aligning scans (CT, MRI) for disease tracking or surgical planning. Satellite imagery is registered to maps for monitoring. Multi-exposure photos are registered for HDR. **Where**: Medical image alignment (patient follow-up), remote sensing (change detection), multi-temporal analysis (tracking land-use changes), face morphing (warping between identities), video stabilization. **Quality check**: Overlay aligned images with partial transparency; features should align. Misalignment indicates poor feature matching or outlier rejection failure. **Extensions**: Non-rigid registration deforms one image elastically to match the other (used for anatomical atlas matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: manual stitching using matches and homography\n",
    "# Why: exposes each step of the panorama pipeline\n",
    "# Ensure homography is available (from Section 10)\n",
    "if 'H' not in locals() or H is None:\n",
    "    raise ValueError(\"Homography H not available; run Section 10 first\")\n",
    "\n",
    "# Create a canvas that can hold both images side-by-side\n",
    "h1, w1 = img_bgr.shape[:2]\n",
    "h2, w2 = img2_bgr.shape[:2]\n",
    "canvas_w = w1 + w2\n",
    "canvas_h = max(h1, h2)\n",
    "\n",
    "# Warp img_bgr into the canvas using the homography\n",
    "warped = cv2.warpPerspective(img_bgr, H, (canvas_w, canvas_h))\n",
    "\n",
    "# Paste the reference image on the left side\n",
    "canvas = warped.copy()\n",
    "canvas[0:h2, 0:w2] = img2_bgr\n",
    "\n",
    "# Blend overlap region for a simple seam reduction\n",
    "blend = cv2.addWeighted(canvas, 0.5, warped, 0.5, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Manual stitch (overlay)\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(blend, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Manual stitch (blended)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: automatic stitching using OpenCV's Stitcher\n",
    "# Why: quick panorama without manual steps\n",
    "stitcher = cv2.Stitcher_create()\n",
    "status, pano = stitcher.stitch([img2_bgr, img_bgr])\n",
    "\n",
    "if status == cv2.Stitcher_OK:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(cv2.cvtColor(pano, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Automatic stitch\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Stitcher failed with status:\", status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c20fb",
   "metadata": {},
   "source": [
    "## 12. Image Stitching\n",
    "**Theory**: Panorama creation stitches together overlapping images into a wider field of view. Manual stitching: find overlaps, compute homographies, place onto canvas. Automatic stitching: pre-calibrated or estimated homographies, blend seams. **Goal**: Demonstrate manual canvas-based stitching and OpenCV's automated Stitcher. **Why**: Panoramas extend field of view beyond single camera capability. Useful for surveying, tourism, heritage documentation, and artistic effects. **Where**: Smartphone panorama mode, satellite map mosaicking (stitching adjacent orbital swaths), microscopy automontage (tiling large samples), augmented reality backgrounds, virtual tours. **Challenges**: Exposure variation (images taken at different times have different brightness), parallax (moving camera reveals objects at different depths), vignetting (brightness drop at image corners). Modern stitchers use blending, exposure compensation, and seam optimization. **Manual approach** shows the core concepts; auto-stitching adds robust feature detection and seam blending."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e12d9",
   "metadata": {},
   "source": [
    "## 13. Blending and Masking\n",
    "**Theory**: After aligning images, blending creates seamless transitions between them. Weighted blending combines images with alpha values: $result = \\alpha \\cdot img_1 + (1-\\alpha) \\cdot img_2$. Masks isolate regions (e.g., circular regions, detected objects). Bitwise operations (AND, OR, NOT) enable selective composition and region extraction. **Goal**: Demonstrate weighted blending, mask-based compositing, and bitwise operations. **Why**: Seams appear when stitching images due to exposure differences or geometric misalignment. Blending smooths transitions. Masks enable selective processing (e.g., face-only augmented reality filters). **Where**: Panorama rendering (feathering seams), image fusion (combining complementary information from multiple sensors), portrait retouching (selective enhancement of face vs. background), augmented reality (compositing virtual objects onto backgrounds), medical image overlay (fusing lesion maps with patient scans). **Practical insight**: Gaussian or soft masks reduce visible stitching artifacts compared to hard rectangular masks. Exposure compensation prior to blending improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56077ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: blend two images with a simple weighted sum\n",
    "# Why: quick way to combine information from two aligned images\n",
    "img_a = img_bgr\n",
    "img_b = cv2.resize(img2_bgr, (w, h))\n",
    "\n",
    "blend_weighted = cv2.addWeighted(img_a, 0.6, img_b, 0.4, 0)\n",
    "\n",
    "# Goal: create a mask to isolate a circular region\n",
    "# Why: masks control where bitwise operations apply\n",
    "mask = np.zeros((h, w), dtype=np.uint8)\n",
    "cv2.circle(mask, (w // 2, h // 2), min(h, w) // 4, 255, -1)\n",
    "\n",
    "# Use mask to extract region from img_a\n",
    "masked_a = cv2.bitwise_and(img_a, img_a, mask=mask)\n",
    "\n",
    "# Invert mask to extract the complement from img_b\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "masked_b = cv2.bitwise_and(img_b, img_b, mask=mask_inv)\n",
    "\n",
    "# Combine the two masked regions\n",
    "combined = cv2.bitwise_or(masked_a, masked_b)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(blend_weighted, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"addWeighted blend\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cv2.cvtColor(combined, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Mask + bitwise combine\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE4216 (Python)",
   "language": "python",
   "name": "ee4216"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
